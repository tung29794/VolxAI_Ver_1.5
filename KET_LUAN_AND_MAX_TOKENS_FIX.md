# Fix: ThÃªm "Káº¿t luáº­n" vÃ  TÄƒng max_tokens cho bÃ i dÃ i

## TÃ³m táº¯t cÃ¡c thay Ä‘á»•i

### 1. ThÃªm section "Káº¿t luáº­n" vÃ o táº¥t cáº£ outline
âœ… Má»i bÃ i viáº¿t giá» Ä‘á»u cÃ³ cáº¥u trÃºc: **Má»Ÿ bÃ i - ThÃ¢n bÃ i - Káº¿t luáº­n**

### 2. TÄƒng max_tokens cho bÃ i viáº¿t dÃ i (3000-4000 tá»«)
âœ… Fix lá»—i `ERR_CONNECTION_RESET 201` khi viáº¿t bÃ i dÃ i vá»›i nhiá»u Ä‘oáº¡n vÄƒn

---

## Chi tiáº¿t thay Ä‘á»•i

### 1ï¸âƒ£ ThÃªm "Káº¿t luáº­n" vÃ o Outline

#### A. Generate Outline (NÃºt "Táº¡o outline")
**File**: `server/routes/ai.ts` (~Line 982)

```typescript
const data = await response.json();
let outline = data.choices[0]?.message?.content?.trim();

if (!outline) {
  res.status(500).json({ error: "No outline generated" });
  return;
}

// Add "Káº¿t luáº­n" section if not present
if (!outline.toLowerCase().includes('káº¿t luáº­n') && !outline.toLowerCase().includes('conclusion')) {
  outline += '\n[h2] Káº¿t luáº­n';
  console.log('âœ… Added "Káº¿t luáº­n" section to outline');
}
```

**Logic:**
- Kiá»ƒm tra outline cÃ³ chá»©a "káº¿t luáº­n" hoáº·c "conclusion" chÆ°a
- Náº¿u chÆ°a cÃ³ â†’ Tá»± Ä‘á»™ng thÃªm `[h2] Káº¿t luáº­n` vÃ o cuá»‘i
- Log Ä‘á»ƒ tracking

#### B. Auto-Generate Outline (No Outline & AI Outline)
**File**: `server/routes/ai.ts` (~Line 1407)

```typescript
if (outlineResponse.ok) {
  const outlineData = await outlineResponse.json();
  autoGeneratedOutline = outlineData.choices[0]?.message?.content?.trim() || "";
  
  // Add "Káº¿t luáº­n" section if not present
  if (autoGeneratedOutline && !autoGeneratedOutline.toLowerCase().includes('káº¿t luáº­n') && !autoGeneratedOutline.toLowerCase().includes('conclusion')) {
    autoGeneratedOutline += '\n[h2] Káº¿t luáº­n';
    console.log('âœ… Added "Káº¿t luáº­n" section to auto-generated outline');
  }
  
  console.log("âœ… Auto-generated outline successfully");
}
```

**Ãp dá»¥ng cho:**
- âœ… No Outline (tá»± Ä‘á»™ng táº¡o outline ngáº§m)
- âœ… AI Outline (khi user khÃ´ng cÃ³ custom outline)

#### Káº¿t quáº£
**TrÆ°á»›c Ä‘Ã¢y:**
```
[h2] Giá»›i thiá»‡u vá» chá»‰ bÃ¡o RSI
[h3] KhÃ¡i niá»‡m RSI
[h3] Lá»‹ch sá»­ phÃ¡t triá»ƒn
[h2] CÃ¡ch tÃ­nh chá»‰ bÃ¡o RSI
[h3] CÃ´ng thá»©c tÃ­nh
[h3] VÃ­ dá»¥ minh há»a
```

**BÃ¢y giá»:**
```
[h2] Giá»›i thiá»‡u vá» chá»‰ bÃ¡o RSI
[h3] KhÃ¡i niá»‡m RSI
[h3] Lá»‹ch sá»­ phÃ¡t triá»ƒn
[h2] CÃ¡ch tÃ­nh chá»‰ bÃ¡o RSI
[h3] CÃ´ng thá»©c tÃ­nh
[h3] VÃ­ dá»¥ minh há»a
[h2] Káº¿t luáº­n  â† Tá»± Ä‘á»™ng thÃªm
```

---

### 2ï¸âƒ£ TÄƒng max_tokens cho bÃ i viáº¿t dÃ i

#### Váº¥n Ä‘á» gáº·p pháº£i
**Lá»—i:** `TypeError: Failed to fetch` vá»›i `ERR_CONNECTION_RESET 201`

**NguyÃªn nhÃ¢n:**
- BÃ i viáº¿t Long (3000-4000 tá»«) vá»›i yÃªu cáº§u 5-6 Ä‘oáº¡n/heading
- max_tokens cá»‘ Ä‘á»‹nh 4096 (OpenAI) vÃ  8192 (Gemini) **khÃ´ng Ä‘á»§**
- AI pháº£i viáº¿t quÃ¡ nhiá»u ná»™i dung â†’ vÆ°á»£t quÃ¡ giá»›i háº¡n â†’ connection reset

#### Giáº£i phÃ¡p: Dynamic max_tokens

**File**: `server/routes/ai.ts` (~Line 1456)

```typescript
// ========== GENERATE ARTICLE USING AI ==========
let content = "";
let finishReason = "";

// Adjust max_tokens based on article length (for both OpenAI and Gemini)
const maxTokensMap: Record<string, number> = {
  short: 4096,   // ~1500-2000 tá»«
  medium: 6000,  // ~2000-2500 tá»«
  long: 8000     // ~3000-4000 tá»«  â† TÄƒng tá»« 4096
};
const maxTokens = maxTokensMap[length?.toLowerCase() || "medium"] || 4096;

const geminiMaxTokensMap: Record<string, number> = {
  short: 8192,   // ~1500-2000 tá»«
  medium: 12000, // ~2000-2500 tá»«  â† TÄƒng tá»« 8192
  long: 16000    // ~3000-4000 tá»«  â† TÄƒng tá»« 8192
};
const geminiMaxTokens = geminiMaxTokensMap[length?.toLowerCase() || "medium"] || 8192;
```

#### OpenAI API (Initial Generation)
```typescript
console.log(`ğŸ“Š Using max_tokens: ${maxTokens} for length: ${length}`);

const response = await fetch("https://api.openai.com/v1/chat/completions", {
  method: "POST",
  body: JSON.stringify({
    model: model === "GPT 5" ? "gpt-4-turbo" : "gpt-3.5-turbo",
    messages: [...],
    temperature: 0.7,
    max_tokens: maxTokens,  // Dynamic: 4096/6000/8000
  }),
});
```

#### Gemini API (Initial Generation)
```typescript
console.log(`ğŸ“Š Using Gemini maxOutputTokens: ${geminiMaxTokens} for length: ${length}`);

const geminiResponse = await fetch(
  `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=${apiKey}`,
  {
    body: JSON.stringify({
      generationConfig: {
        temperature: 0.7,
        maxOutputTokens: geminiMaxTokens,  // Dynamic: 8192/12000/16000
        topP: 0.95,
        topK: 40
      },
    }),
  }
);
```

#### OpenAI Continuation
```typescript
const continuationResponse = await fetch("https://api.openai.com/v1/chat/completions", {
  body: JSON.stringify({
    model: model === "GPT 5" ? "gpt-4-turbo" : "gpt-3.5-turbo",
    messages: continuationMessages,
    temperature: 0.7,
    max_tokens: maxTokens,  // Same as initial generation
  }),
});
```

#### Gemini Continuation
```typescript
const geminiContinuationResponse = await fetch(
  `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=${apiKey}`,
  {
    body: JSON.stringify({
      generationConfig: {
        temperature: 0.7,
        maxOutputTokens: geminiMaxTokens,  // Same as initial generation
        topP: 0.95,
        topK: 40
      },
    }),
  }
);
```

---

## Báº£ng so sÃ¡nh max_tokens

### OpenAI (GPT-3.5 & GPT-4)

| Äá»™ dÃ i | Sá»‘ tá»« | max_tokens CÅ¨ | max_tokens Má»šI | TÄƒng |
|--------|-------|---------------|----------------|------|
| Short  | 1500-2000 | 4096 | 4096 | - |
| Medium | 2000-2500 | 4096 | **6000** | +1904 (46%) |
| Long   | 3000-4000 | 4096 | **8000** | +3904 (95%) |

### Gemini (2.0 Flash)

| Äá»™ dÃ i | Sá»‘ tá»« | maxOutputTokens CÅ¨ | maxOutputTokens Má»šI | TÄƒng |
|--------|-------|-------------------|---------------------|------|
| Short  | 1500-2000 | 8192 | 8192 | - |
| Medium | 2000-2500 | 8192 | **12000** | +3808 (46%) |
| Long   | 3000-4000 | 8192 | **16000** | +7808 (95%) |

---

## Táº¡i sao tÄƒng nhiá»u nhÆ° váº­y?

### TÃ­nh toÃ¡n cho Long article (3000-4000 tá»«)

**YÃªu cáº§u:**
- No Outline: **5-6 Ä‘oáº¡n** cho má»—i heading (H2 vÃ  H3)
- Má»—i Ä‘oáº¡n: **120+ tá»«**
- 7 H2 sections, má»—i H2 cÃ³ 4 H3 subsections

**Æ¯á»›c tÃ­nh ná»™i dung:**
```
7 H2 Ã— 5 Ä‘oáº¡n Ã— 120 tá»« = 4,200 tá»« (chá»‰ H2)
7 H2 Ã— 4 H3 Ã— 5 Ä‘oáº¡n Ã— 120 tá»« = 16,800 tá»« (H3)
Total: ~21,000 tá»« ná»™i dung

Vá»›i HTML tags, formatting, examples:
~25,000-30,000 tokens cáº§n thiáº¿t
```

**Thá»±c táº¿:**
- OpenAI 8000 tokens â‰ˆ 6,000 tá»« output
- Gemini 16000 tokens â‰ˆ 12,000 tá»« output
- Vá»›i continuation logic, cÃ³ thá»ƒ táº¡o Ä‘Æ°á»£c bÃ i Ä‘á»§ dÃ i

---

## Lá»£i Ã­ch

### 1. Cáº¥u trÃºc bÃ i viáº¿t hoÃ n chá»‰nh
- âœ… **Má»Ÿ bÃ i**: CÃ¡c H2 Ä‘áº§u tiÃªn giá»›i thiá»‡u chá»§ Ä‘á»
- âœ… **ThÃ¢n bÃ i**: PhÃ¡t triá»ƒn ná»™i dung chi tiáº¿t
- âœ… **Káº¿t luáº­n**: Tá»•ng káº¿t vÃ  Ä‘á» xuáº¥t (tá»± Ä‘á»™ng thÃªm)

### 2. Fix lá»—i bÃ i viáº¿t dÃ i
- âœ… KhÃ´ng cÃ²n `ERR_CONNECTION_RESET`
- âœ… AI cÃ³ Ä‘á»§ max_tokens Ä‘á»ƒ viáº¿t bÃ i dÃ i
- âœ… Continuation logic hoáº¡t Ä‘á»™ng á»•n Ä‘á»‹nh hÆ¡n

### 3. Cháº¥t lÆ°á»£ng bÃ i viáº¿t tá»‘t hÆ¡n
- âœ… BÃ i dÃ i cÃ³ nhiá»u chi tiáº¿t hÆ¡n
- âœ… Äá»§ tokens Ä‘á»ƒ viáº¿t 5-6 Ä‘oáº¡n/heading
- âœ… KhÃ´ng bá»‹ cáº¯t giá»¯a chá»«ng

---

## Testing Checklist

### Test "Káº¿t luáº­n" Section

#### 1. Generate Outline (NÃºt "Táº¡o outline")
- [ ] Táº¡o outline má»›i â†’ Kiá»ƒm tra cÃ³ `[h2] Káº¿t luáº­n` á»Ÿ cuá»‘i
- [ ] Outline cÃ³ sáºµn "conclusion" â†’ KhÃ´ng thÃªm duplicate
- [ ] Outline Ä‘Ã£ cÃ³ "Káº¿t luáº­n" â†’ KhÃ´ng thÃªm duplicate

#### 2. No Outline
- [ ] Viáº¿t bÃ i vá»›i No Outline + Short
- [ ] Kiá»ƒm tra console: "Added 'Káº¿t luáº­n' section to auto-generated outline"
- [ ] Kiá»ƒm tra bÃ i viáº¿t cÃ³ section "Káº¿t luáº­n" vá»›i ná»™i dung

#### 3. AI Outline
- [ ] Chá»n AI Outline nhÆ°ng khÃ´ng nháº­p custom outline
- [ ] Há»‡ thá»‘ng tá»± táº¡o outline
- [ ] Kiá»ƒm tra cÃ³ `[h2] Káº¿t luáº­n` á»Ÿ cuá»‘i

### Test max_tokens cho bÃ i dÃ i

#### 1. Short Article (1500-2000 tá»«)
- [ ] Viáº¿t vá»›i OpenAI â†’ Check log: "Using max_tokens: 4096"
- [ ] Viáº¿t vá»›i Gemini + Google Search â†’ Check: "Using Gemini maxOutputTokens: 8192"
- [ ] BÃ i viáº¿t hoÃ n thÃ nh khÃ´ng bá»‹ lá»—i

#### 2. Medium Article (2000-2500 tá»«)
- [ ] OpenAI: max_tokens = 6000
- [ ] Gemini: maxOutputTokens = 12000
- [ ] BÃ i viáº¿t Ä‘á»§ Ä‘á»™ dÃ i

#### 3. Long Article (3000-4000 tá»«) - CRITICAL TEST
- [ ] OpenAI: max_tokens = 8000
- [ ] Gemini: maxOutputTokens = 16000
- [ ] **KhÃ´ng cÃ²n lá»—i `ERR_CONNECTION_RESET 201`**
- [ ] BÃ i viáº¿t Ä‘áº¡t 3000-4000 tá»«
- [ ] Má»—i heading cÃ³ 5-6 Ä‘oáº¡n nhÆ° yÃªu cáº§u
- [ ] Continuation logic hoáº¡t Ä‘á»™ng (náº¿u cáº§n)

#### 4. Continuation Tests
- [ ] BÃ i viáº¿t dÃ i cáº§n continuation
- [ ] OpenAI continuation dÃ¹ng maxTokens (8000)
- [ ] Gemini continuation dÃ¹ng geminiMaxTokens (16000)
- [ ] KhÃ´ng bá»‹ timeout

---

## Console Logs Ä‘á»ƒ kiá»ƒm tra

### Khi táº¡o outline:
```
âœ… Added "Káº¿t luáº­n" section to outline
âœ… Added "Káº¿t luáº­n" section to auto-generated outline
```

### Khi generate article:
```
ğŸ“Š Using max_tokens: 8000 for length: long
ğŸ“Š Using Gemini maxOutputTokens: 16000 for length: long
```

### Khi continuation:
```
ğŸ“ Gemini continuation received: +1500 words, finishReason: MAX_TOKENS â†’ length
```

---

## Build Status
âœ… Build completed successfully
- Client: 940.10 kB
- Server: 232.77 kB
- No errors

---

## Deployment Notes

### 1. Deploy ngay
Thay Ä‘á»•i nÃ y fix lá»—i nghiÃªm trá»ng (connection reset) cho bÃ i dÃ i

### 2. Monitor logs
Kiá»ƒm tra console logs sau deploy:
- "Added 'Káº¿t luáº­n'" messages
- "Using max_tokens" values
- "Using Gemini maxOutputTokens" values

### 3. User feedback
Thu tháº­p feedback vá»:
- BÃ i viáº¿t cÃ³ Ä‘á»§ section "Káº¿t luáº­n" chÆ°a
- BÃ i dÃ i (3000-4000 tá»«) cÃ³ cÃ²n lá»—i khÃ´ng
- Cháº¥t lÆ°á»£ng ná»™i dung cÃ³ cáº£i thiá»‡n khÃ´ng

---

## Technical Details

### max_tokens vs Words
**Rough estimation:**
- 1 token â‰ˆ 0.75 words (English)
- 1 token â‰ˆ 0.5-0.6 words (Vietnamese - more tokens per word)

**Examples:**
- 4096 tokens â‰ˆ 2,400-3,000 Vietnamese words
- 8000 tokens â‰ˆ 4,800-6,000 Vietnamese words
- 16000 tokens â‰ˆ 9,600-12,000 Vietnamese words

### Continuation Logic
Vá»›i max_tokens tÄƒng cao:
- Ãt cáº§n continuation hÆ¡n
- Má»—i continuation viáº¿t Ä‘Æ°á»£c nhiá»u hÆ¡n
- Giáº£m sá»‘ láº§n gá»i API
- Tiáº¿t kiá»‡m tokens vÃ  thá»i gian

---

## Risks & Mitigations

### Risk 1: Token costs tÄƒng
**Mitigation:** 
- Chá»‰ tÄƒng cho bÃ i dÃ i (Long)
- Short/Medium váº«n há»£p lÃ½
- Continuation Ã­t hÆ¡n â†’ giáº£m total cost

### Risk 2: Response time cháº­m hÆ¡n
**Mitigation:**
- Acceptable trade-off Ä‘á»ƒ fix lá»—i
- BÃ i dÃ i vá»‘n Ä‘Ã£ tá»‘n thá»i gian
- User Ä‘Ã£ chá»n "Long" = cháº¥p nháº­n Ä‘á»£i lÃ¢u hÆ¡n

### Risk 3: API limits
**Mitigation:**
- OpenAI GPT-3.5/4 há»— trá»£ 8000+ tokens
- Gemini 2.0 Flash há»— trá»£ 16000+ tokens
- Äá»u trong limits cho phÃ©p

---

## Future Improvements

### 1. Dynamic "Káº¿t luáº­n" content
Thay vÃ¬ chá»‰ thÃªm heading, cÃ³ thá»ƒ:
- Generate ná»™i dung káº¿t luáº­n tá»± Ä‘á»™ng
- Tá»•ng há»£p key points tá»« bÃ i viáº¿t
- ThÃªm CTA hoáº·c recommendations

### 2. Smart max_tokens
TÃ­nh toÃ¡n chÃ­nh xÃ¡c hÆ¡n dá»±a trÃªn:
- Sá»‘ lÆ°á»£ng H2/H3 trong outline
- Sá»‘ Ä‘oáº¡n/heading yÃªu cáº§u
- Äá»™ dÃ i má»—i Ä‘oáº¡n
- Formula: `maxTokens = (h2Count Ã— h2Paragraphs + h3Count Ã— h3Paragraphs) Ã— paragraphWords Ã— 1.5`

### 3. Progress indicator
Hiá»ƒn thá»‹ % complete khi viáº¿t bÃ i dÃ i:
- "Äang viáº¿t pháº§n 1/7..."
- "ÄÃ£ hoÃ n thÃ nh 45%..."
- "Äang tiáº¿p tá»¥c viáº¿t..."

---

## Summary

âœ… **Fixed 2 major issues:**
1. Added "Káº¿t luáº­n" section to all outlines (structure completeness)
2. Increased max_tokens for long articles (fix ERR_CONNECTION_RESET)

âœ… **Impact:**
- Better article structure (má»Ÿ bÃ i - thÃ¢n bÃ i - káº¿t luáº­n)
- No more crashes on long articles
- Higher quality content with more paragraphs

âœ… **Ready for production!**
