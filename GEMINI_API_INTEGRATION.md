# Gemini API Integration v·ªõi Google Search

## T·ªïng quan

T√≠ch h·ª£p Gemini API ƒë·ªÉ h·ªó tr·ª£ t√≠nh nƒÉng "Tham kh·∫£o th√™m ki·∫øn th·ª©c tr√™n Google t√¨m ki·∫øm". Khi user ch·ªçn checkbox n√†y, h·ªá th·ªëng s·∫Ω:
- ‚úÖ Force s·ª≠ d·ª•ng Gemini 2.5 Flash
- ‚úÖ G·ªçi Gemini API v·ªõi Google Search Retrieval
- ‚úÖ AI c√≥ th·ªÉ tham kh·∫£o ki·∫øn th·ª©c realtime t·ª´ Google

## Logic ho·∫°t ƒë·ªông

### 1. Ch·ªçn AI Provider

```typescript
if (useGoogleSearch) {
  // Use Google AI (Gemini)
  const googleApiKeys = await query(
    `SELECT api_key FROM api_keys
     WHERE provider = 'google-ai' AND category = 'content' AND is_active = TRUE
     LIMIT 1`
  );
  apiKey = googleApiKeys[0].api_key;
  provider = 'google-ai';
} else {
  // Use OpenAI (default)
  const apiKeys = await query(
    `SELECT api_key FROM api_keys
     WHERE provider = 'openai' AND category = 'content' AND is_active = TRUE
     LIMIT 1`
  );
  apiKey = apiKeys[0].api_key;
  provider = 'openai';
}
```

### 2. G·ªçi API ph√π h·ª£p

#### Khi useGoogleSearch = true (Gemini):

```typescript
const geminiResponse = await fetch(
  `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=${apiKey}`,
  {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      contents: [
        {
          parts: [
            {
              text: geminiPrompt // Combined system + user prompt
            }
          ]
        }
      ],
      generationConfig: {
        temperature: 0.7,
        maxOutputTokens: 8192, // Cao h∆°n OpenAI (4096)
        topP: 0.95,
        topK: 40
      },
      tools: [
        {
          googleSearchRetrieval: {
            dynamicRetrievalConfig: {
              mode: "MODE_DYNAMIC",
              dynamicThreshold: 0.3
            }
          }
        }
      ]
    }),
  }
);

const geminiData = await geminiResponse.json();
content = geminiData.candidates?.[0]?.content?.parts?.[0]?.text?.trim() || "";
finishReason = geminiData.candidates?.[0]?.finishReason === "MAX_TOKENS" ? "length" : "stop";
```

**ƒêi·ªÉm quan tr·ªçng:**
- ‚úÖ Model: `gemini-2.0-flash-exp` (Gemini 2.5 Flash experimental)
- ‚úÖ maxOutputTokens: 8192 (g·∫•p ƒë√¥i OpenAI)
- ‚úÖ **tools: googleSearchRetrieval** - T√≠nh nƒÉng quan tr·ªçng nh·∫•t, cho ph√©p AI t√¨m ki·∫øm Google
- ‚úÖ dynamicThreshold: 0.3 - AI t·ª± quy·∫øt ƒë·ªãnh khi n√†o c·∫ßn search

#### Khi useGoogleSearch = false (OpenAI):

```typescript
const response = await fetch("https://api.openai.com/v1/chat/completions", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    Authorization: `Bearer ${apiKey}`,
  },
  body: JSON.stringify({
    model: model === "GPT 5" ? "gpt-4-turbo" : "gpt-3.5-turbo",
    messages: [
      {
        role: "system",
        content: systemPrompt,
      },
      {
        role: "user",
        content: userPrompt,
      },
    ],
    temperature: 0.7,
    max_tokens: 4096,
  }),
});

const data = await response.json();
content = data.choices[0]?.message?.content?.trim() || "";
finishReason = data.choices[0]?.finish_reason || "";
```

### 3. Continuation Logic (c·∫£ 2 provider)

Logic continuation ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·ªÉ h·ªó tr·ª£ c·∫£ Gemini v√† OpenAI:

```typescript
while (finishReason === "length" && attemptCount < maxAttempts) {
  // Check outline completion
  if (outlineToCheck && checkOutlineCompletion(content, outlineToCheck)) {
    break;
  }
  
  attemptCount++;
  
  if (useGoogleSearch && provider === 'google-ai') {
    // Continue with Gemini
    const geminiContinuationResponse = await fetch(...);
    continuationText = geminiContinuationData.candidates?.[0]?.content?.parts?.[0]?.text;
    finishReason = geminiContinuationData.candidates?.[0]?.finishReason === "MAX_TOKENS" ? "length" : "stop";
  } else {
    // Continue with OpenAI
    const continuationResponse = await fetch(...);
    continuationText = continuationData.choices[0]?.message?.content;
    finishReason = continuationData.choices[0]?.finish_reason;
  }
  
  if (continuationText) {
    content += "\n\n" + continuationText;
  }
}
```

## Gemini API Response Format

### Success Response:
```json
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "N·ªôi dung b√†i vi·∫øt..."
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "groundingMetadata": {
        "groundingChunks": [...],
        "groundingSupports": [...],
        "webSearchQueries": ["query 1", "query 2"]
      }
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 150,
    "candidatesTokenCount": 2500,
    "totalTokenCount": 2650
  }
}
```

### Finish Reasons:
- `STOP` - Ho√†n th√†nh b√¨nh th∆∞·ªùng
- `MAX_TOKENS` - ƒê·∫°t gi·ªõi h·∫°n token (trigger continuation)
- `SAFETY` - B·ªã ch·∫∑n b·ªüi safety filter
- `RECITATION` - Ph√°t hi·ªán n·ªôi dung c√≥ b·∫£n quy·ªÅn

## Database Setup

### API Keys Table:
```sql
SELECT id, provider, category, description 
FROM api_keys 
WHERE provider = 'google-ai';
```

Result:
```
id | provider   | category | description
9  | google-ai  | content  | Gemini
```

### AI Prompts Table:
```sql
SELECT id, feature_name, display_name, is_active 
FROM ai_prompts 
WHERE feature_name = 'generate_article';
```

Result:
```
id | feature_name      | display_name              | is_active
14 | generate_article  | T·∫°o b√†i vi·∫øt ho√†n ch·ªânh  | 1
```

## Logging

Console logs ƒë·ªÉ theo d√µi:

```
üîç Using Google AI (Gemini) with search knowledge
‚úÖ Using database prompt for generate_article
üìù System prompt preview: You are a professional SEO content writer...
üìù User prompt preview: Write a comprehensive article about: "..."
‚úÖ Gemini response received, length: ~1200 words
üìù Article was cut off, continuing... (Attempt 2/10)
üìä Outline check: 5/7 H2 sections completed
‚ö†Ô∏è 2 sections still missing from outline
üìä Article length after continuation: ~2400 words
‚úÖ All outline sections completed
```

## Token Calculation

V√¨ Gemini API response format kh√°c OpenAI, t√¥i ƒë√£ ƒë·ªïi sang estimated tokens:

```typescript
// Estimate tokens since we don't have raw API response
const estimatedTokens = Math.ceil(content.length / 4); // 1 token ‚âà 4 chars
const titleTokens = Math.ceil((title?.length || 0) / 4);
const totalActualTokens = estimatedTokens + titleTokens;
```

**Note:** Trong production, c√≥ th·ªÉ l∆∞u `usageMetadata` t·ª´ Gemini response ƒë·ªÉ t√≠nh ch√≠nh x√°c.

## Testing

### Test Case 1: Kh√¥ng ch·ªçn Google Search
- User ch·ªçn Model: GPT 4.1 MINI
- Kh√¥ng check "Tham kh·∫£o Google"
- ‚úÖ K·∫øt qu·∫£: S·ª≠ d·ª•ng OpenAI API

### Test Case 2: C√≥ ch·ªçn Google Search
- User ch·ªçn b·∫•t k·ª≥ Model n√†o
- Check "Tham kh·∫£o Google"
- ‚úÖ K·∫øt qu·∫£: Force Gemini 2.5 Flash, s·ª≠ d·ª•ng Gemini API v·ªõi Google Search

### Test Case 3: B√†i d√†i c·∫ßn continuation
- Check "Tham kh·∫£o Google"
- Outline d√†i (7-10 H2)
- ‚úÖ K·∫øt qu·∫£: Gemini API ƒë∆∞·ª£c g·ªçi nhi·ªÅu l·∫ßn cho ƒë·∫øn khi ho√†n th√†nh

## So s√°nh OpenAI vs Gemini

| Feature | OpenAI | Gemini |
|---------|--------|--------|
| Model | gpt-3.5-turbo / gpt-4-turbo | gemini-2.0-flash-exp |
| Max Tokens | 4096 | 8192 |
| Google Search | ‚ùå Kh√¥ng | ‚úÖ C√≥ (googleSearchRetrieval) |
| Token Cost | Tr·∫£ ph√≠ cao | Tr·∫£ ph√≠ th·∫•p h∆°n |
| Response Format | choices[0].message.content | candidates[0].content.parts[0].text |
| System Prompt | Ri√™ng bi·ªát | K·∫øt h·ª£p v·ªõi user prompt |

## Known Issues & Limitations

1. **‚ö†Ô∏è Gemini API key c·∫ßn c√≥ quy·ªÅn s·ª≠ d·ª•ng Google Search**
   - C·∫ßn enable "Grounded Generation" trong Google AI Studio
   - API key ph·∫£i c√≥ billing account active

2. **‚ö†Ô∏è Token estimation kh√¥ng ch√≠nh x√°c 100%**
   - Hi·ªán t·∫°i d√πng `length / 4` ƒë·ªÉ estimate
   - C√¢n nh·∫Øc l∆∞u `usageMetadata` t·ª´ response

3. **‚ö†Ô∏è Safety filters c√≥ th·ªÉ ch·∫∑n n·ªôi dung**
   - Gemini c√≥ safety filters nghi√™m ng·∫∑t h∆°n OpenAI
   - C·∫ßn handle finishReason = "SAFETY"

## Next Steps

- [ ] Test v·ªõi Google AI API key th·ª±c t·∫ø
- [ ] Monitor Gemini API quota usage
- [ ] Implement accurate token counting v·ªõi usageMetadata
- [ ] Add retry logic cho Gemini API errors
- [ ] Handle safety filter blocks gracefully
- [ ] Consider caching Google Search results

## Files Changed

1. `server/routes/ai.ts` - Added Gemini API integration
   - Line ~1389: API provider selection logic
   - Line ~1395: Gemini API call with googleSearchRetrieval
   - Line ~1550: Gemini continuation logic

## Build Status

‚úÖ Build successful
- Client: ‚úì
- Server: ‚úì (218.05 kB)

## Deployment Notes

1. Upload `dist/server/node-build.mjs` to server
2. Ensure Google AI API key is active in database
3. Restart Node.js application
4. Test with checkbox "Tham kh·∫£o Google" enabled
5. Monitor logs for Gemini API calls
